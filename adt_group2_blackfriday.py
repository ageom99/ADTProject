# -*- coding: utf-8 -*-
"""ADT_Group2_BlackFriday.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lzof4yzPtHOeWEwPw3y-RQ82GKT-gucY
"""

pip install 'pymongo[srv]'

pip install dnspython

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pymongo
from pymongo import MongoClient
import csv
import json
import sys, getopt, pprint

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

client = pymongo.MongoClient("mongodb+srv://adtproject:adt123@adt.3r1ht.mongodb.net/ADT?retryWrites=true&w=majority")

client.stats

client.list_database_names()

db = client.BlackFridaySales

list (db.list_collections())

train = db["Train"]
test = db["Test"]

train_df = pd.DataFrame(list(train.find()))

test_df = pd.DataFrame(list(test.find()))

df=train_df

df

df_train = df.drop(columns=['_id'])

df_train

## The dataset has the following columns  User_ID, Product_ID, Gender, Age, Occupation, City_Category,
## Stay_In_Current_City_Years, Marital_Status, Product_Category_1, Product_Category_2, Product_Category_3 and Purchase

df_train.shape
## The dataframe has 550068 rows and 12 columns.

df_train.describe()

convert_dict = {'User_ID': np.int64,
                'Occupation': np.int64,
                'Marital_Status': np.int64,
                'Product_Category_1': np.int64,
                'Product_Category_2': np.float64,
                'Product_Category_3':np.float64,
                'Purchase': np.int64
               }
df_train = df_train.astype(convert_dict)

df_train.astype({'User_ID': 'int64'}).dtypes

df_train.astype({'Occupation': 'int64'}).dtypes

df_train.info()
## Using the info function we can see below that there are a few null values present in Product_Category_3 column
## And we can also see the types of variables present in these columns.

df_train.isnull().sum()
## The isnull().sum() function will help in finding all the null values present in the columns 
## There are 173638 missing values and 383247 missing values in the Product_category_2 and Product_category_3 columns

## The graph below shows the occurrences of the gender in the dataset.

plt.figure(figsize=(10, 6))
sns.countplot(data=df_train, x='Gender', palette='cubehelix')

## the count of male is higher than female

plt.figure(figsize=(10, 6))
sns.barplot(x='Gender',y='Marital_Status',data=df_train)
## The below visualization shows us that the female gender is slightly higher compared to the male gender.

plt.figure(figsize=(10, 6))
sns.barplot(x='Gender',y='Purchase',data=df_train)
## Higher purchases have been done by the male gender as compared to the female.

plt.figure(figsize=(10, 6))
sns.barplot(x='Occupation',y='Purchase',data=df_train);
## Occupation has a direct effect on the purchases done by the customer and the occupation codes 12,15,17 have higher purchases.

plt.figure(figsize=(10, 6))
sns.barplot(x='Occupation',y='Purchase',hue='Gender',data=df_train)
## In this graph it can be seen that the female gender in the occupation 18 with higher purchases compared to others.

plt.figure(figsize=(10, 6))
sns.boxplot(data=df_train, x="Gender", y="Purchase")
## Using boxplot we can detect the presence of outliers in the data.

plt.figure(figsize=(10, 6))
sns.boxplot(data=df_train, x="Occupation", y="Purchase")
## The purchase column has outliers which may effect the performance of the machine learning models.

plt.figure(figsize=(10, 6))
sns.boxplot(data=df_train, x="Age", y="Purchase")
## We can see below that the Age with Purchases again have some outliers present in them.

plt.figure(figsize=(10, 6))
sns.boxplot(data=df_train, x="Product_Category_1", y="Purchase")
## There are outliers present in the Product category as well.

df_train

df_train['Product_ID'] = df_train['Product_ID'].str.replace('P00', '')
ss = StandardScaler()
df_train['Product_ID'] = ss.fit_transform(df_train['Product_ID'].values.reshape(-1, 1))
## Replacing ''P00'' with no value and scaling the ProductID column.

df_train

df_train.drop(['Product_Category_3'],axis=1,inplace=True)
## There are more than 50 percent missing values present in the Product_category_column so we will drop that column.

df_train.describe

df_train['Product_Category_2']=df_train['Product_Category_2'].fillna(df_train['Product_Category_2'].mean())
## The missing data in the product category 2 column have been imputed using mean.

df_train.describe()

df_train.isnull().sum()
## As we can see the missing values have been successfully imputed and now there are no null values present in the dataset.

df_train

cat_cols=['Gender','City_Category','Age']
le=LabelEncoder()
for i in cat_cols:
    df_train[i]=le.fit_transform(df_train[i])
df_train.dtypes
## The label encoding technique will now replace all the categorical variables to numeric for easier computation.

cat_cols=['Gender','City_Category','Age']
le=LabelEncoder()
for i in cat_cols:
    df_train[i]=le.fit_transform(df_train[i])
df_train.dtypes
## The label encoding technique will now replace all the categorical variables to numeric for easier computation.

df_train['Stay_In_Current_City_Years']=df_train['Stay_In_Current_City_Years'].replace('4+','4')
## Values in the Stay_In_Current_City_Years column has been changed from 4+ to 4

df_train['Gender']=df_train['Gender'].astype(int)
df_train['Age']=df_train['Age'].astype(int)
df_train['Stay_In_Current_City_Years']=df_train['Stay_In_Current_City_Years'].astype(int)
## The gender, Age and Stay_In_Current_City_Years values are changed to integer types.

df_train['City_Category']=df_train['City_Category'].astype('category')
## The type of city_category has been changed from int to category.

df_train

rows=3
cols=3
fig, ax=plt.subplots(nrows=rows,ncols=cols,figsize=(10,4))
col=df.columns
index=2
for i in range(rows):
    for j in range(cols):
        sns.distplot(df_train[col[index]],ax=ax[i][j])
        index=index+1
        
plt.tight_layout()
## The distribution plot helps us to detect the skewness of the data.Below as it can be seen that the purchase column

df_train['Purchase']=np.log(df_train['Purchase'])
## The log transformation will help us transform the data and change the data to normal distribution

df_train = pd.get_dummies(df_train)
df_train.head()
## The get_dummies() function is used to convert categorical variable into dummy/indicator variables.

X=df_train.drop(labels=['Purchase'],axis=1)                         
Y=df['Purchase']
X.head()
## The data is split into X and Y where independent and dependent variables have been separated.

Y

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)
print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)
## The data has been split into Train and test.

model=LinearRegression()
model.fit(X_train,Y_train)

Y_predict=model.predict(X_test)

score=r2_score(Y_test,Y_predict)
mae=mean_absolute_error(Y_test,Y_predict)
mse=mean_squared_error(Y_test,Y_predict)
rmse=(np.sqrt(mean_squared_error(Y_test,Y_predict)))
print('r2_score: ',score)
print('mean_absolute_error: ',mae)
print('mean_squared_error: ',mse)
print('root_mean_squared_error: ',rmse)

DT=DecisionTreeRegressor(max_depth=9)
DT.fit(X_train,Y_train)

train_preds=DT.predict(X_train)
#predicting on test
test_preds=DT.predict(X_test)

RMSE_train=(np.sqrt(metrics.mean_squared_error(Y_train,train_preds)))
RMSE_test=(np.sqrt(metrics.mean_squared_error(Y_test,test_preds)))
print("RMSE TrainingData = ",str(RMSE_train))
print("RMSE TestData = ",str(RMSE_test))
print('-'*50)
print('RSquared value on train:',DT.score(X_train, Y_train))
print('RSquared value on test:',DT.score(X_test, Y_test))

RF=RandomForestRegressor().fit(X_train,Y_train)

#predicting train
train_preds1=RF.predict(X_train)
#predicting on test
test_preds1=RF.predict(X_test)

RMSE_train=(np.sqrt(metrics.mean_squared_error(Y_train,train_preds1)))
RMSE_test=(np.sqrt(metrics.mean_squared_error(Y_test,test_preds1)))
print("RMSE TrainingData = ",str(RMSE_train))
print("RMSE TestData = ",str(RMSE_test))
print('-'*50)
print('RSquared value on train:',RF.score(X_train, Y_train))
print('RSquared value on test:',RF.score(X_test, Y_test))

test_df
## Loading test dataset

test_df.info()

df_test = test_df.drop(columns=['_id'])

convert_dict = {'User_ID': np.int64,
                'Occupation': np.int64,
                'Marital_Status': np.int64,
                'Product_Category_1': np.int64,
                'Product_Category_2': np.float64,
                'Product_Category_3': np.float64
               }
df_test = df_test.astype(convert_dict)

df_test.isnull().sum()

df_test.info()

df_test['Product_ID'] = df_test['Product_ID'].str.replace('P00', '')
ss = StandardScaler()
df_test['Product_ID'] = ss.fit_transform(df_test['Product_ID'].values.reshape(-1, 1))
## The 'P00' value has been replaced int he ProductId column and the column has been scaled.

df_test.drop(['Product_Category_3'],axis=1,inplace=True)
## As the Product_Category_3 column in the train set had been removed. Same has been done here aswell.

df_test['Product_Category_2']=df_test['Product_Category_2'].fillna(df_test['Product_Category_2'].mean())
## Product_Category_2 has been imputed with mean

df_test.isnull().sum()
## As we see there are no null values in the test dataframe as well.

df_test

cat_cols=['Gender','City_Category','Age']
le=LabelEncoder()
for i in cat_cols:
    df_test[i]=le.fit_transform(df_test[i])
df_test.dtypes
## The label encoding technique will now replace all the categorical variables to numeric for easier computation.

df_test['Stay_In_Current_City_Years']=df_test['Stay_In_Current_City_Years'].replace('4+','4')
## The 4+ value in the Stay_In_Current_City_Years have been replaced with only 4.

df_test['Gender']=df_test['Gender'].astype(int)
df_test['Age']=df_test['Age'].astype(int)
df_test['Stay_In_Current_City_Years']=df_test['Stay_In_Current_City_Years'].astype(int)
df_test['City_Category']=df_test['City_Category'].astype('category')
## The values in the test set have been converted to integer types as done in the train set.

df_test= pd.get_dummies(df_test)
## Dummies are created for the test set.

df_test.head()

df_train.shape
# train data shape

df_test.shape
# test data shape

df_train

df_test

test_preds= RF.predict(df_test)
len(test_preds)

final_test_df = pd.DataFrame(list(test.find()))

final_test = final_test_df.drop(columns=['_id'])

convert_dict = {'User_ID': np.int64,
                'Occupation': np.int64,
                'Marital_Status': np.int64,
                'Product_Category_1': np.int64,
                'Product_Category_2': np.float64,
                'Product_Category_3': np.float64
               }
final_test = final_test.astype(convert_dict)

ID_info= final_test[["User_ID","Product_ID"]]
ID_info.head()
## Using User_Id and Product_Id from the test set.

predictions= pd.DataFrame(test_preds, columns=["Purchase"])
predictions["User_ID"]= ID_info["User_ID"]
predictions["Product_ID"]= ID_info["Product_ID"]
predictions.head()
## Predictions have been save in the form of a dataframe

predictions.to_csv('BlackFridayPredictions.csv', index=False)
## Finally converted the prediction into csv format.