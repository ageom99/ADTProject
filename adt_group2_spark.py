# -*- coding: utf-8 -*-
"""ADT_Group2_Spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ehq0NAAclE3V1JkKYHUesW86JZI6K97W
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz

!tar xf /content/spark-3.0.3-bin-hadoop3.2.tgz

!pip install -q findspark

!pip install pyspark==3.0.3

!pip install pandasql

!pip install -U featuretools

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.3-bin-hadoop3.2"

import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark.context import SparkContext
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import date, timedelta, datetime
import time
import pandas as pd
import pandasql
from pandasql import sqldf
import matplotlib.pyplot as plt

# spark = SparkSession.builder.master("local[*]").getOrCreate()

#Initialization of a Spark Session
spark = SparkSession.builder.appName("PySparkExample")\
    .config ("spark.sql.shuffle.partitions", "50") \
    .config("spark.driver.maxResultSize","10g") \
    .config ("spark.sql.execution.arrow.enabled", "true")\
    .getOrCreate()

#Using Lambda for easy SQL queries
pysqldf = lambda q: sqldf(q, globals())

spark_time = []
sql_time = []

start = time.time()

# Load Data in Spark
df_train = spark.read.option("header",True).option("inferSchema", True).format("csv").load("/content/train.csv")
df_test = spark.read.option("header",True).option("inferSchema", True).format("csv").load("/content/test.csv")

end = time.time()
print(end-start)

start = time.time()

# Load Data in for SQL using Pandas
train_df = pd.read_csv("/content/train.csv")
test_df = pd.read_csv("/content/test.csv") 

end = time.time()
print(end - start)

df_train.show()

df_test.show()

train_df

test_df

df_train.dtypes

df_train.columns

df_train.createOrReplaceTempView("train")
df_test.createOrReplaceTempView("test")

start = time.time()

spark.sql("SELECT Gender FROM train LIMIT 10")
#df_train.select("Gender").show(10)

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT Gender 
       FROM train_df 
       LIMIT 10;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

start = time.time()

spark.sql("SELECT * FROM train WHERE Gender = 'M' LIMIT 5000")
#df_train.where("Gender = 'M'").show(5000)

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT * FROM train_df WHERE Gender = "M" LIMIT 5000;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

start = time.time()

updatedtest_spark = spark.sql("SELECT User_id, Product_id, Gender FROM test")
updatedtest_spark.collect()

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT User_id, Product_id, Gender FROM test_df;"""

updatedtest_sql = pysqldf(q);
print(updatedtest_sql);

end = time.time()
sql_time.append(end-start)

start = time.time()

updatedtrain_spark = spark.sql("SELECT User_ID,Age,Occupation ,City_Category,Stay_In_Current_City_Years FROM train")
updatedtrain_spark.collect()

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT User_ID,Age,Occupation ,City_Category,Stay_In_Current_City_Years FROM train_df;"""

updatedtrain_sql = pysqldf(q);
print(updatedtrain_sql);

end = time.time()
sql_time.append(end-start)

updatedtrain_spark.createOrReplaceTempView("updatedtrain")
updatedtest_spark.createOrReplaceTempView("updatedtest")

start = time.time()

spark.sql("SELECT updatedtrain.User_ID,updatedtrain.Age,updatedtrain.Occupation,\
          updatedtrain.City_Category,updatedtrain.Stay_In_Current_City_Years\
          FROM updatedtrain \
          INNER JOIN updatedtest\
          ON updatedtest.User_ID = updatedtrain.User_ID").show(5000)

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT updatedtrain_sql.User_ID,updatedtrain_sql.Age,updatedtrain_sql.Occupation,
        updatedtrain_sql.City_Category,updatedtrain_sql.Stay_In_Current_City_Years
        FROM updatedtrain_sql 
        INNER JOIN updatedtest_sql
        ON updatedtest_sql.User_ID = updatedtrain_sql.User_ID
        LIMIT 5000;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

start = time.time()

spark.sql("SELECT * FROM updatedtest JOIN updatedtrain USING (User_ID)").show(5000)

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT * FROM updatedtest_sql JOIN updatedtrain_sql USING (User_ID) LIMIT 5000;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

start = time.time()

spark.sql("SELECT sum(length(Gender)) AS AllCharactersLength from train").show()

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT sum(length(Gender)) AS AllCharactersLength from train_df;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

start = time.time()

spark.sql("SELECT COUNT(test.Product_id) AS num_orders\
          FROM test\
          INNER JOIN train ON test.User_id = train.User_id\
          GROUP BY test.Age").show()

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT COUNT(test_df.Product_id) AS num_orders\
          FROM test_df\
          INNER JOIN train_df ON test_df.User_id = train_df.User_id\
          GROUP BY test_df.Age;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

start = time.time()

spark.sql("SELECT train.Product_ID, train.City_Category, train.Age\
            FROM train\
            LEFT JOIN test ON train.User_ID = test.User_ID\
            ORDER BY train.Age;").show(5000)

end = time.time()
spark_time.append(end-start)

start = time.time()

q = """SELECT train_df.Product_ID, train_df.City_Category, train_df.Age\
        FROM train_df\
        LEFT JOIN test_df ON train_df.User_ID = test_df.User_ID\
        ORDER BY train_df.Age LIMIT 5000;"""

print(pysqldf(q));

end = time.time()
sql_time.append(end-start)

plt.plot(spark_time, color='blue', label='Spark Time')
plt.plot(sql_time, color='red', label='SQL Time')
plt.ylabel('Time Taken')
plt.xlabel('Query Complexity')
plt.legend()
plt.title('Spark VS SQL')
plt.show()